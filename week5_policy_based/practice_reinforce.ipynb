{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_reinforce.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvy04_8dOVLu",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCE in TensorFlow\n",
        "\n",
        "Just like we did before for q-learning, this time we'll design a neural network to learn `CartPole-v0` via policy gradient (REINFORCE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgbTpfkeOVLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "20c0f8ea-ec13-47e0-e9ae-10fe971e69dd"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    %tensorflow_version 1.x\n",
        "    \n",
        "    if not os.path.exists('.setup_complete'):\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "        !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIl-Ilx7OVL0",
        "colab_type": "text"
      },
      "source": [
        "A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZDZXjQ3OVL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "ad538f5d-f34f-4667-8cfc-a1bb091758ef"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1d8dc89400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS7UlEQVR4nO3df6yeZZ3n8feH/uKXDiDH2m0LZbSrYTZjMWcRopthEGeQuIuTuAZ2g40h6WxSE0nM7sJssqPJkMzEHZklO0u2ExhxdUR21KFhmGUASUaTFShYa6GiVeu0TUvLr4JjRNp+949zFR9Ly3nOL06vc96v5Mlz39/7up/ne8WHj3evcz/npKqQJPXjpNluQJI0MQa3JHXG4JakzhjcktQZg1uSOmNwS1JnZiy4k1ye5Mkk25NcP1PvI0nzTWbiPu4kC4DvA+8HdgGPAFdX1RPT/maSNM/M1BX3hcD2qvpRVf0CuAO4cobeS5LmlYUz9LrLgZ0D+7uAdx9v8Nlnn12rVq2aoVYkqT87duzg6aefzrGOzVRwjyvJOmAdwDnnnMOmTZtmqxVJOuGMjo4e99hMLZXsBlYO7K9otVdU1YaqGq2q0ZGRkRlqQ5LmnpkK7keA1UnOS7IYuArYOEPvJUnzyowslVTVwSQfB+4FFgC3VdXjM/FekjTfzNgad1XdA9wzU68vSfOV35yUpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktSZKf3psiQ7gBeBQ8DBqhpNchbwZWAVsAP4SFU9N7U2JUlHTMcV929X1ZqqGm371wMPVNVq4IG2L0maJjOxVHIlcHvbvh340Ay8hyTNW1MN7gL+PsmjSda12tKq2tO29wJLp/gekqQBU1rjBt5bVbuTvBm4L8n3Bg9WVSWpY53Ygn4dwDnnnDPFNiRp/pjSFXdV7W7P+4CvARcCTyVZBtCe9x3n3A1VNVpVoyMjI1NpQ5LmlUkHd5LTkrzhyDbwO8BWYCOwtg1bC9w11SYlSb80laWSpcDXkhx5nb+qqv+b5BHgziTXAj8BPjL1NiVJR0w6uKvqR8A7j1F/BnjfVJqSJB2f35yUpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOjNucCe5Lcm+JFsHamcluS/JD9rzma2eJDcn2Z5kS5J3zWTzkjQfDXPF/Tng8qNq1wMPVNVq4IG2D/ABYHV7rANumZ42JUlHjBvcVfUPwLNHla8Ebm/btwMfGqh/vsZ8CzgjybLpalaSNPk17qVVtadt7wWWtu3lwM6Bcbta7VWSrEuyKcmm/fv3T7INSZp/pvzDyaoqoCZx3oaqGq2q0ZGRkam2IUnzxmSD+6kjSyDteV+r7wZWDoxb0WqSpGky2eDeCKxt22uBuwbqH213l1wEHBhYUpEkTYOF4w1I8iXgEuDsJLuAPwT+GLgzybXAT4CPtOH3AFcA24GfAR+bgZ4laV4bN7ir6urjHHrfMcYWsH6qTUmSjs9vTkpSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6sy4wZ3ktiT7kmwdqH0qye4km9vjioFjNyTZnuTJJL87U41L0nw1zBX354DLj1G/qarWtMc9AEnOB64CfqOd8z+TLJiuZiVJQwR3Vf0D8OyQr3clcEdVvVRVP2bsr71fOIX+JElHmcoa98eTbGlLKWe22nJg58CYXa32KknWJdmUZNP+/fun0IYkzS+TDe5bgLcCa4A9wJ9O9AWqakNVjVbV6MjIyCTbkKT5Z1LBXVVPVdWhqjoM/AW/XA7ZDawcGLqi1SRJ02RSwZ1k2cDu7wFH7jjZCFyVZEmS84DVwMNTa1GSNGjheAOSfAm4BDg7yS7gD4FLkqwBCtgB/D5AVT2e5E7gCeAgsL6qDs1M65I0P40b3FV19THKt77G+BuBG6fSlCTp+PzmpCR1xuCWpM4Y3JLUGYNbkjpjcEtSZ8a9q0Sab37xT8/z8+f3smDxKZw2cu5styO9isEtHeXAP27hH7/xRRYsPpXT3rwKgJMWLubc31rLwiWnzm5zEga3dFyHfvEzXtj1BAAnLVxCHTo4yx1JY1zjlqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktSZcYM7ycokDyZ5IsnjST7R6mcluS/JD9rzma2eJDcn2Z5kS5J3zfQkJGk+GeaK+yDwyao6H7gIWJ/kfOB64IGqWg080PYBPsDYX3dfDawDbpn2riVpHhs3uKtqT1U91rZfBLYBy4ErgdvbsNuBD7XtK4HP15hvAWckWTbtnUvSPDWhNe4kq4ALgIeApVW1px3aCyxt28uBnQOn7Wq1o19rXZJNSTbt379/gm1L0vw1dHAnOR34CnBdVb0weKyqCqiJvHFVbaiq0aoaHRkZmcipkjSvDRXcSRYxFtpfrKqvtvJTR5ZA2vO+Vt8NrBw4fUWrSZKmwTB3lQS4FdhWVZ8dOLQRWNu21wJ3DdQ/2u4uuQg4MLCkIkmaomH+As57gGuA7ybZ3Gp/APwxcGeSa4GfAB9px+4BrgC2Az8DPjatHUvSPDducFfVN4Ec5/D7jjG+gPVT7EuSdBx+c1KSOmNwS1JnDG5J6ozBLQ04fPBlnvn+/3tV/cy3jrJgyamz0JH0aga3NKDqMC+98PSr6otPP4uTFgxzE5Y08wxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4Jakzgzzx4JXJnkwyRNJHk/yiVb/VJLdSTa3xxUD59yQZHuSJ5P87kxOQJLmm2F+T+VB4JNV9ViSNwCPJrmvHbupqv7b4OAk5wNXAb8B/DPg/iT/vKoOTWfjkjRfjXvFXVV7quqxtv0isA1Y/hqnXAncUVUvVdWPGftr7xdOR7OSpAmucSdZBVwAPNRKH0+yJcltSc5steXAzoHTdvHaQS9JmoChgzvJ6cBXgOuq6gXgFuCtwBpgD/CnE3njJOuSbEqyaf/+/RM5VZLmtaGCO8kixkL7i1X1VYCqeqqqDlXVYeAv+OVyyG5g5cDpK1rtV1TVhqoararRkZGRqcxBkuaVYe4qCXArsK2qPjtQXzYw7PeArW17I3BVkiVJzgNWAw9PX8uSNL8Nc1fJe4BrgO8m2dxqfwBcnWQNUMAO4PcBqurxJHcCTzB2R8p67yiRpOkzbnBX1TeBHOPQPa9xzo3AjVPoS5J0HH5zUpI6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTPD/FpXqWuHDx/muuuuY+fOneOOXbQgrP+tszh9yYJfqd9xxx18449uHer91q9fz2WXXTapXqVhGNya86qK+++/n23bto079uTFC7n23VexeNGZVI39g3ThSb9g27Zt/M3fPjbU+33wgx+cUr/SeAxu6SjPvbyUzfv/NS/XEgCWnfxjDtcjs9yV9EsGtzTgUC1k8/OXcMppp79S2/vzcznw8tmz2JX0q/zhpHSUQ7X4V/YP1hL2v7RilrqRXm2YPxZ8cpKHk3wnyeNJPt3q5yV5KMn2JF9OsrjVl7T97e34qpmdgjR9QnHyST/9ldqi/Jzlp2yfpY6kVxvmivsl4NKqeiewBrg8yUXAnwA3VdXbgOeAa9v4a4HnWv2mNk7qQjjIm1/+Ki88u40XD+zktAXPc95pWzl1wYuz3Zr0imH+WHABRy5BFrVHAZcC/67Vbwc+BdwCXNm2Af4a+B9J0l5HOqG99PIhrvuzv6L4Eme98RT+1W+eSyi2/WT/bLcmvWKoH04mWQA8CrwN+HPgh8DzVXWwDdkFLG/by4GdAFV1MMkB4E3A08d7/b179/KZz3xmUhOQxlNVPPPMM0OPP1wFFM8c+Cf+5htPTPj97r33Xp599tkJnycN2rt373GPDRXcVXUIWJPkDOBrwDum2lSSdcA6gOXLl3PNNddM9SWlYzp8+DC33nor+/bte13e7+KLL+bqq69+Xd5Lc9cXvvCF4x6b0O2AVfV8kgeBi4EzkixsV90rgN1t2G5gJbAryULg14BXXe5U1QZgA8Do6Gi95S1vmUgr0tAOHTrEggULxh84Td74xjfi51lTtWjRouMeG+aukpF2pU2SU4D3A9uAB4EPt2Frgbva9sa2Tzv+dde3JWn6DHPFvQy4va1znwTcWVV3J3kCuCPJHwHfBo78Iodbgf+dZDvwLHDVDPQtSfPWMHeVbAEuOEb9R8CFx6j/HPi309KdJOlV/OakJHXG4JakzvhLpjTnJeGyyy7j7W9/++vyfueee+7r8j6avwxuzXknnXQSN99882y3IU0bl0okqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmeG+WPBJyd5OMl3kjye5NOt/rkkP06yuT3WtHqS3Jxke5ItSd4105OQpPlkmN/H/RJwaVX9NMki4JtJ/q4d+49V9ddHjf8AsLo93g3c0p4lSdNg3CvuGvPTtruoPeo1TrkS+Hw771vAGUmWTb1VSRIMucadZEGSzcA+4L6qeqgdurEth9yUZEmrLQd2Dpy+q9UkSdNgqOCuqkNVtQZYAVyY5F8ANwDvAP4lcBbwnyfyxknWJdmUZNP+/fsn2LYkzV8Tuqukqp4HHgQur6o9bTnkJeAvgQvbsN3AyoHTVrTa0a+1oapGq2p0ZGRkct1L0jw0zF0lI0nOaNunAO8Hvndk3TpJgA8BW9spG4GPtrtLLgIOVNWeGelekuahYe4qWQbcnmQBY0F/Z1XdneTrSUaAAJuB/9DG3wNcAWwHfgZ8bPrblqT5a9zgrqotwAXHqF96nPEFrJ96a5KkY/Gbk5LUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTOpqtnugSQvAk/Odh8z5Gzg6dluYgbM1XnB3J2b8+rLuVU1cqwDC1/vTo7jyaoane0mZkKSTXNxbnN1XjB35+a85g6XSiSpMwa3JHXmRAnuDbPdwAyaq3Obq/OCuTs35zVHnBA/nJQkDe9EueKWJA1p1oM7yeVJnkyyPcn1s93PRCW5Lcm+JFsHamcluS/JD9rzma2eJDe3uW5J8q7Z6/y1JVmZ5MEkTyR5PMknWr3ruSU5OcnDSb7T5vXpVj8vyUOt/y8nWdzqS9r+9nZ81Wz2P54kC5J8O8ndbX+uzGtHku8m2ZxkU6t1/VmcilkN7iQLgD8HPgCcD1yd5PzZ7GkSPgdcflTteuCBqloNPND2YWyeq9tjHXDL69TjZBwEPllV5wMXAevb/za9z+0l4NKqeiewBrg8yUXAnwA3VdXbgOeAa9v4a4HnWv2mNu5E9glg28D+XJkXwG9X1ZqBW/96/yxOXlXN2gO4GLh3YP8G4IbZ7GmS81gFbB3YfxJY1raXMXafOsD/Aq4+1rgT/QHcBbx/Ls0NOBV4DHg3Y1/gWNjqr3wugXuBi9v2wjYus937ceazgrEAuxS4G8hcmFfrcQdw9lG1OfNZnOhjtpdKlgM7B/Z3tVrvllbVnra9F1jatrucb/tn9AXAQ8yBubXlhM3APuA+4IfA81V1sA0Z7P2VebXjB4A3vb4dD+3PgP8EHG77b2JuzAuggL9P8miSda3W/Wdxsk6Ub07OWVVVSbq9dSfJ6cBXgOuq6oUkrxzrdW5VdQhYk+QM4GvAO2a5pSlL8kFgX1U9muSS2e5nBry3qnYneTNwX5LvDR7s9bM4WbN9xb0bWDmwv6LVevdUkmUA7Xlfq3c13ySLGAvtL1bVV1t5TswNoKqeBx5kbAnhjCRHLmQGe39lXu34rwHPvM6tDuM9wL9JsgO4g7Hlkv9O//MCoKp2t+d9jP2f7YXMoc/iRM12cD8CrG4/+V4MXAVsnOWepsNGYG3bXsvY+vCR+kfbT70vAg4M/FPvhJKxS+tbgW1V9dmBQ13PLclIu9ImySmMrdtvYyzAP9yGHT2vI/P9MPD1agunJ5KquqGqVlTVKsb+O/p6Vf17Op8XQJLTkrzhyDbwO8BWOv8sTslsL7IDVwDfZ2yd8b/Mdj+T6P9LwB7gZcbW0q5lbK3wAeAHwP3AWW1sGLuL5ofAd4HR2e7/Neb1XsbWFbcAm9vjit7nBvwm8O02r63Af231XwceBrYD/wdY0uont/3t7fivz/YchpjjJcDdc2VebQ7faY/Hj+RE75/FqTz85qQkdWa2l0okSRNkcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1Jn/DwVFbplVpLdxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbQM4t19OVL4",
        "colab_type": "text"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnaqDDK1OVL5",
        "colab_type": "text"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h95p6i7xOVL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# create input variables. We only need <s,a,R> for REINFORCE\n",
        "states = tf.placeholder('float32', (None,)+state_dim, name=\"states\")\n",
        "actions = tf.placeholder('int32', name=\"action_ids\")\n",
        "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whfm48W-OVL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = tf.keras.models.Sequential()\n",
        "network.add(tf.keras.layers.InputLayer(state_dim))\n",
        "network.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "network.add(tf.keras.layers.Dense(n_actions, activation='linear'))\n",
        "logits = network(states)\n",
        "policy = tf.nn.softmax(logits)\n",
        "log_policy = tf.nn.log_softmax(logits)\n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2F4t0-FOVL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utility function to pick action in one given state\n",
        "def get_action_proba(s):\n",
        "    return policy.eval({states: [s]})[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouXF0lBsOVMA",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "\n",
        "Following the REINFORCE algorithm, we can define our objective as follows: \n",
        "\n",
        "$$ \\hat J \\approx { 1 \\over N } \\sum_{s_i,a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMEY_PgSOVMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), actions], axis=-1)\n",
        "log_policy_for_actions = tf.gather_nd(log_policy, indices)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E55FrlODOVMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# policy objective as in the last formula. please use mean, not sum.\n",
        "# note: you need to use log_policy_for_actions to get log probabilities for actions taken.\n",
        "\n",
        "J = tf.reduce_mean((log_policy_for_actions * cumulative_rewards), axis=-1)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3i9_W32OVMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# regularize with entropy\n",
        "entropy = -tf.reduce_mean(policy*log_policy)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG_sjE8oOVMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all network weights\n",
        "all_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "\n",
        "# weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
        "loss = -J - 0.1*entropy\n",
        "\n",
        "update = tf.train.AdamOptimizer().minimize(loss, var_list=all_weights)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVes9eyQOVMJ",
        "colab_type": "text"
      },
      "source": [
        "### Computing cumulative rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w-aaYSUOVMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    take a list of immediate rewards r(s,a) for the whole session \n",
        "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
        "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
        "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "\n",
        "    cumulative_rewards = list()\n",
        "    for timestamp in range(1,len(rewards)+1):\n",
        "        if timestamp == 1:\n",
        "            cumulative_rewards.insert(-timestamp, rewards[-timestamp])\n",
        "        else:\n",
        "            cumulative_rewards.insert(-timestamp, rewards[-timestamp] + gamma * cumulative_rewards[-timestamp+1])\n",
        "\n",
        "    return cumulative_rewards"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOWq_Ox7OVML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e8270a9-784f-4362-fbbd-97b3e8f5222d"
      },
      "source": [
        "assert len(get_cumulative_rewards(range(100))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw9OkmdeOVMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(_states, _actions, _rewards):\n",
        "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
        "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
        "    update.run({states: _states, actions: _actions,\n",
        "                cumulative_rewards: _cumulative_rewards})"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yboLEPb-OVMP",
        "colab_type": "text"
      },
      "source": [
        "### Playing the game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrvjSEvjOVMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
        "\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probas = get_action_proba(s)\n",
        "\n",
        "        a = np.random.choice(range(len(action_probas)),p=action_probas)\n",
        "\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    train_step(states, actions, rewards)\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    # return sum(rewards)\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7b-o-77OVMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e6b724cb-6758-45c9-9254-0dca3ccf3cfd"
      },
      "source": [
        "s = tf.InteractiveSession()\n",
        "s.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "    rewards = [sum(generate_session(env)[2]) for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mean reward:27.030\n",
            "mean reward:59.840\n",
            "mean reward:149.670\n",
            "mean reward:387.130\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGOgZD1WOVMV",
        "colab_type": "text"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwgY5IdsOVMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CupNN59gOVMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfvOtK3qOVMY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31147b2e-605e-4c69-a311-cfa55b2eed59"
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'stsbar@gmail.com', 'OhNwM6n481cfsn7V')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Y1F6lkOVMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# That's all, thank you for your attention!\n",
        "# Not having enough? There's an actor-critic waiting for you in the honor section.\n",
        "# But make sure you've seen the videos first."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}